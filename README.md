This project implements a vision-guided manipulation system for a UR5e robotic arm equipped with an end-effector camera and a 2F-85 gripper. The controller uses the onboard camera to detect objects on a table via a colour/shape-based cube detection function, and then autonomously clears the table by picking the objects and dropping them into a designated tray. The system builds a full kinematic chain from the robot’s URDF file using KinPy, enabling both forward and inverse kinematics to control the arm in task space. Once a cube is detected, the robot aligns itself by adjusting joint angles based on pixel offsets between the cube location and the camera’s image centre. After alignment, the controller performs a grasping routine that moves the end effector downward, closes the gripper, lifts the object, and computes an inverse-kinematics solution to move to the tray location. The object is then released into the tray by opening the gripper. This creates a complete perception-to-action pipeline that combines camera-based object detection, kinematic control, and autonomous pick-and-place behaviour for clearing objects from a workspace.
